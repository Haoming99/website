<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Haoming Li</title>

    <meta name="author" content="Haoming Li">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Haoming Li
                </p>
                <p>I am a Ph.D. student of Computer Science at Arizona State University advised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=k2suuZgAAAAJ&view_op=list_works&sortby=pubdate">Yezhou (YZ) Yang</a></p>

                <p>Before it, I was a research assistant advised by <a href="https://www.cis.upenn.edu/~kostas/" target="_blank">Prof. Kostas Daniilidis</a> in the <a href="https://www.grasp.upenn.edu/" target="_blank">General Robotics, Automation, Sensing and Perception (GRASP) Laboratory</a> at the <a href="https://www.seas.upenn.edu/" target="_blank">University of Pennsylvania</a>. I completed my master's degree in Electrical Engineering at <a href="https://www.seas.upenn.edu/" target="_blank">UPenn</a> co-supervised by <a href="https://nbfigueroa.github.io/" target="_blank">Prof. Nadia Figueroa</a> and <a href="https://pratikac.github.io/" target="_blank">Prof. Pratik Chaudhari</a>. I recieved my bachelor's degree in Electrical and Information Engrineering at the <a href="https://en.cugb.edu.cn/" target="_blank">China University of Geosciences, Beijing</a> advised by <a href="https://people.ucas.ac.cn/~_yanliu?language=en" target="_blank">Prof. Yan Liu</a> and <a href="https://people.ucas.ac.cn/~kmchen" target="_blank">Prof. Keming Chen</a>. </p>

                <p>You can reach me at <em>lihaomingforreal [at] gmail [dot] com</em>.</p>

                <p style="text-align:center">
                  <a href="mailto:lihaomingforreal@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/cv_202503.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/haoming-li-2b5946261/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/lhm.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/lhm.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research goal is to develop computer vision systems that can intelligently interact with the physical world. To achieve it, I focus on learning the visual prior by equipping large-scale vision generative models with a spatial and physical understanding of the world for interaction with the environment.
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>




              
  <tr onmouseout="switchImage('project1-image', 'images/grasping.gif')" 
      onmouseover="switchImage('project1-image', 'images/net_work.png')">
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div id="image-container">
        <img id="project1-image" src="images/grasping.gif" width="300">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
          <strong>
      <a href="https://github.com/Haoming99/DiffGrasp/tree/main" target="_blank">
        DiffGrasp: Diffusion-based 3D Shape Completion for Robust Robotic Grasping
      </a>
    </strong>
      <br>
      <strong>Haoming Li</strong>*, <a href="https://jiangwenpl.github.io/">Wen Jiang</a>*, <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>
      <br>
      Paper in perparation 
      <br>
      <a href="https://github.com/Haoming99/DiffGrasp/tree/main" target="_blank">
        Code
      </a>
      <br>
      <p></p>
      <p>We propose DiffGrasp, a diffusion-based 3D shape completion method for robust robotic grasping. 
      Our method compresses 3D shapes into a lower-dimensional latent space and takes the shape completion as a reverse diffusion process conditioned on a partial point cloud to produce multimodal but realistic complete 3D shapes. 
      At runtime, our method takes a single view depth image as input and generates accurate grasp poses based on the completed 3D shape. 
    </td>
  </tr>



            

<tr onmouseout="dualfont_stop()" onmouseover="dualfont_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <!-- Replace the src with your own image -->
    <img src='images/two_obs.gif' width="300">
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://www.arxiv.org/abs/2505.02294" target="_blank">
<strong>RNBF: Real-Time RGB-D Based Neural Barrier Functions for Safe Robotic Navigation</strong>
    </a>
    <br>
      <a href="https://scholar.google.com/citations?user=REEqyVUAAAAJ&hl=en">Satyajeet Das</a>, <a href="https://www.linkedin.com/in/yifan-xue-523168178/">Yifan Xue</a>, <strong>Haoming Li</strong>, <a href="https://nbfigueroa.github.io/">Nadia Figueroa</a>
    <br>
    <em>IROS 2025 under review</em>
    <br>
    <a href="https://www.arxiv.org/abs/2505.02294">Arxiv</a>
    <br>
    <p></p>
    <p>We introduce a novel framework for real-time control barrier function construction in unknown environments using an RGB-D camera. RNBF-CBF-QP enables reactive obstacle avoidance in static and quasi-static environments, without pre-training, while handling sensor noise. </p>
  </td>
</tr>



            
<tr onmouseout="dualfont_stop()" onmouseover="dualfont_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <!-- Replace the src with your own image -->
    <img src='images/thesis.png' width="300">
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="images/thesis.pdf">
      <span class="papertitle">Towards Generalizable Robust Safe Robotic Systems via
Lipschitz Regularization</span>
    </a>
    <br>
    Advisor:  <a href="http://www.cs.cmu.edu/~asrivats/">Nadia Figueroa</a>,
    Co-advisor: <a href="https://pratikac.github.io/">Pratik Chaudhari</a>
    <br>
    <em>Master' Thesis</em>, 2024     <a href="images/thesis.pdf">
      <br>
      <a href="images/thesis.pdf">Paper</a>
    </a>
    <br>
    <p></p>
    <p>We propose a semantic-informed neural SLAM to enhance the generalization thus produce precise scene reconstruction and
robust camera tracking. We explore the benefits of semantic information to
refine both mapping and tracking processes. Additionally, we introduce a novel weight normalization and regularization to
increase network robustness against noises and perturbation. We evaluate our method on the Replica and the Neural RGBD datasets, demonstrating
its effectiveness in robotic navigation.</p>
  </td>
</tr>





<tr onmouseout="dualfont_stop()" onmouseover="dualfont_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <!-- Replace the src with your own image -->
    <img src='images/changedetection.png' width="300">
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://ieeexplore.ieee.org/document/9779962">
      <span class="papertitle">A Divided Spatial and Temporal Context Network for Remote Sensing Change Detection</span>
    </a>
    <br>
    Advisor: <a href="http://www.aircas.cas.cn/sourcedb/cn/expert/yjy/202306/t20230614_6778122.html">Keming Chen</a>
    <br>
    <em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em>, 2022
    <br>
    <a href="https://ieeexplore.ieee.org/document/9779962">Paper</a>
    <p></p>
    <p> We introduce a transformer-based method to globally model spatial and temporal context in remote sensing change detection tasks. By introducing Transformer into modeling the global
context, the network can produce accurate segmentation masks for multitemporal remote sensing images. </p>
  </td>
</tr>



            

<tr onmouseout="dualfont_stop()" onmouseover="dualfont_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <!-- Replace the src with your own image -->
    <img src='images/ice.png' width="300">
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://ieeexplore.ieee.org/document/9321072/authors#authors">
      <span class="papertitle">Ice Crevasse Detection with Ground
Penetrating Radar using Faster R-CNN</span>
    </a>
    <br>
                <a href="https://people.ucas.ac.cn/~_yanliu?language=en">Yan Liu</a>,
                <strong>Haoming Li</strong>, 
                Mingzhe Hunang,
                <a href="https://people.ucas.ac.cn/~0007255?language=en">Deyuan Chen</a>,
                Bo Zhao,
    <br>
    <em>IEEE International Conference on Signal Processing</em>, 2020
    <br>
    <a href="https://ieeexplore.ieee.org/document/9321072/authors#authors">Paper</a>
    <p></p>
    <p>We propose an ice crevasse detection method based on Faster R-CNN achieving an
accuracy above 95% for safe navigation.</p>
  </td>
</tr>

            





          </tbody></table>
            

            
        </td>
      </tr>
    </table>
  </body>
</html>
